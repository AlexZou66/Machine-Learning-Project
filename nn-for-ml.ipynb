{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n#df = pd.read_csv('/kaggle/input/tencentforuse/edit')\ndf=pd.read_csv('/kaggle/input/newclustering/newclasstering.csv')\n#nlp=pd.read_csv('/kaggle/input/nlp-for-ml/NLP.CSV')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-28T07:40:03.082507Z","iopub.execute_input":"2022-01-28T07:40:03.082824Z","iopub.status.idle":"2022-01-28T07:40:03.160997Z","shell.execute_reply.started":"2022-01-28T07:40:03.082730Z","shell.execute_reply":"2022-01-28T07:40:03.160145Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nk = np.loadtxt('/kaggle/input/nlpforuse/a.csv',dtype=np.float,delimiter=' ',unpack=False)\nk=k[:2941]\nprint(len(k))","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:03.162902Z","iopub.execute_input":"2022-01-28T07:40:03.163495Z","iopub.status.idle":"2022-01-28T07:40:09.592227Z","shell.execute_reply.started":"2022-01-28T07:40:03.163449Z","shell.execute_reply":"2022-01-28T07:40:09.591213Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df = df[['year','topic','status','1','2','play cat']]","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:09.593318Z","iopub.execute_input":"2022-01-28T07:40:09.593516Z","iopub.status.idle":"2022-01-28T07:40:09.607765Z","shell.execute_reply.started":"2022-01-28T07:40:09.593492Z","shell.execute_reply":"2022-01-28T07:40:09.606711Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df[['status']]=df[['status']].fillna(round(df.mean()))\ndf=df.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:09.608925Z","iopub.execute_input":"2022-01-28T07:40:09.609396Z","iopub.status.idle":"2022-01-28T07:40:09.623862Z","shell.execute_reply.started":"2022-01-28T07:40:09.609360Z","shell.execute_reply":"2022-01-28T07:40:09.622954Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"#from scipy import stats\n#t=stats.zscore(df) \n#t=abs(t) \n#filtered_entries = (t < 3).all(axis=1)\n#new_df = df[filtered_entries]\nnew_df=df","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:09.626417Z","iopub.execute_input":"2022-01-28T07:40:09.626869Z","iopub.status.idle":"2022-01-28T07:40:09.633923Z","shell.execute_reply.started":"2022-01-28T07:40:09.626826Z","shell.execute_reply":"2022-01-28T07:40:09.633218Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#y_mean_std=(new_df['play number'].mean(),new_df['play number'].std())\n#normalized_df=(new_df-new_df.mean())/new_df.std()\nnew_df['status']=(new_df['status']-new_df['status'].mean())/new_df['status'].std()\nnew_df['1']=(new_df['1']-new_df['1'].mean())/new_df['1'].std()\nnew_df['2']=(new_df['2']-new_df['2'].mean())/new_df['2'].std()\n#new_df['year'].value_counts()\n#from sklearn.preprocessing import LabelBinarizer\n#y = OneHotEncoder().fit_transform(x).toarray()\n#print(y)\n#new_df\n#print(y_mean_std)\nnew_df","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:09.635029Z","iopub.execute_input":"2022-01-28T07:40:09.635357Z","iopub.status.idle":"2022-01-28T07:40:09.664229Z","shell.execute_reply.started":"2022-01-28T07:40:09.635332Z","shell.execute_reply":"2022-01-28T07:40:09.663369Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"need to deleting outliners","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import transforms, datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nBATCH_SIZE=8","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:09.665725Z","iopub.execute_input":"2022-01-28T07:40:09.666000Z","iopub.status.idle":"2022-01-28T07:40:11.832132Z","shell.execute_reply.started":"2022-01-28T07:40:09.665963Z","shell.execute_reply":"2022-01-28T07:40:11.831246Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nk=torch.tensor(k)\n#print(k.size())\nyear= LabelBinarizer().fit_transform(new_df['year'])\nyear=torch.tensor(year)\ntopic=LabelBinarizer().fit_transform(new_df['topic'])\ntopic=torch.tensor(topic)\n#print(topic[0])\n#print(year.size())\nx= torch.tensor(new_df.iloc[:, 2:-1].values)\n#print(x)\ny=torch.tensor(new_df.iloc[:, -1].values)\ny = y.view(-1, 1)\n#print(y)\ndata = torch.cat((x, year), 1)\ndata = torch.cat((data, topic), 1)\ndata = torch.cat((data, k), 1)\ndata = torch.cat((data, y), 1)\n#print(data.size())\ntrain, test = train_test_split(data, test_size=0.1)\n#print(train.size()[1])\nx_train, y_train=torch.split(train, [train.size()[1]-1,1],dim=1)\nx_test, y_test=torch.split(test, [train.size()[1]-1,1],dim=1)\n#year_train = LabelBinarizer().fit_transform(train['year'])\n#x_train = torch.tensor(train.iloc[:, 0:-2].values)\n#y_train=torch.tensor(train.iloc[:, -1].values)\n#year_train=torch.tensor(year_train)\n#x_train = torch.cat((x_train, year_train), 1)\n\n#year_test = LabelBinarizer().fit_transform(test['year'])\n#x_test = torch.tensor(test.iloc[:, 0:-2].values)\n#y_test=torch.tensor(test.iloc[:, -1].values)\n#year_test=torch.tensor(year_test)\n#x_test = torch.cat((x_test, year_test), 1)\n\nprint(x_train.size())\n#x_test","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:11.833494Z","iopub.execute_input":"2022-01-28T07:40:11.833725Z","iopub.status.idle":"2022-01-28T07:40:12.032548Z","shell.execute_reply.started":"2022-01-28T07:40:11.833696Z","shell.execute_reply":"2022-01-28T07:40:12.031649Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"dataset1 = TensorDataset(x_train, y_train)\ntrainset = DataLoader(dataset1, batch_size=BATCH_SIZE, shuffle=True)\ndataset2 = TensorDataset(x_test, y_test)\ntestset = DataLoader(dataset2, batch_size=len(y_test), shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:12.033773Z","iopub.execute_input":"2022-01-28T07:40:12.034565Z","iopub.status.idle":"2022-01-28T07:40:12.039404Z","shell.execute_reply.started":"2022-01-28T07:40:12.034526Z","shell.execute_reply":"2022-01-28T07:40:12.038534Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(4483, 1024)\n        self.fc2 = nn.Linear(1024, 256)\n        #self.fc3 = nn.Linear(256, 128)\n        #self.fc4 = nn.Linear(128, 128)\n        #self.fc5 = nn.Linear(128, 128)\n        #self.fc6 = nn.Linear(128, 128)\n        #self.fc7 = nn.Linear(128, 128)\n        self.fc8 = nn.Linear(256, 128)\n        self.fc9 = nn.Linear(128, 3)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        #x = F.relu(self.fc3(x))\n        #x = F.relu(self.fc4(x))\n        #x = F.relu(self.fc5(x))\n        #x = F.relu(self.fc6(x))  \n        #x = F.relu(self.fc7(x))\n        x = F.relu(self.fc8(x))\n        x = self.fc9(x)\n        return x\n\nnet = Net()\nprint(net)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:12.040477Z","iopub.execute_input":"2022-01-28T07:40:12.040689Z","iopub.status.idle":"2022-01-28T07:40:12.096161Z","shell.execute_reply.started":"2022-01-28T07:40:12.040662Z","shell.execute_reply":"2022-01-28T07:40:12.095514Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\n#optimizer = optim.Adam(net.parameters(), lr=1e-3)\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:12.097131Z","iopub.execute_input":"2022-01-28T07:40:12.097447Z","iopub.status.idle":"2022-01-28T07:40:12.101647Z","shell.execute_reply.started":"2022-01-28T07:40:12.097421Z","shell.execute_reply":"2022-01-28T07:40:12.100813Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"for epoch in range(200): # 3 full passes over the data\n    for idx, data in enumerate(trainset):  # `data` is a batch of data\n        #if idx > 0:\n            #continue\n        X, y = data    # X is the batch of features, y is the batch of targets.\n        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n        #print('x:',X)\n        X=X.clone().detach().float()\n        output = net(X.view(-1,4483))  # pass in the reshaped batch (recall they are 28x28 atm) view(-1,3)\n        #print(y)\n        #y = y.clone().detach().float()\n        #loss = F.nll_loss(output, y.view(-1, 1))\n        y = y.type(torch.LongTensor)\n        y = y.squeeze(1) \n        #print(output,y)\n        loss = criterion(output, y)  # calc and grab the loss value torch.nn.MSELoss\n        #print(loss)\n        loss.backward()  # apply this loss backwards thru the network's parameters\n        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n    #print(output) \n    #print(output,y)\n    if epoch % 5 == 0:\n        #print(y,output)\n        print(epoch,loss)  # print loss. We hope loss (a measure of wrong-ness) declines! \n    if loss<=0.01:\n        break\nprint('end')\n        ","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:40:12.102692Z","iopub.execute_input":"2022-01-28T07:40:12.102885Z","iopub.status.idle":"2022-01-28T07:41:57.489191Z","shell.execute_reply.started":"2022-01-28T07:40:12.102861Z","shell.execute_reply":"2022-01-28T07:41:57.488307Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"s=0\nn=0\nfor data in testset:\n    X,y=data\n    X=X.clone().detach().float()\n    output = net(X.view(-1,4483))  # pass in the reshaped batch (recall they are 28x28 atm) view(-1,3)\n        #print(output)\n        #print(y)\n    #y = y.clone().detach().float()\n    y = y.type(torch.LongTensor)\n    y = y.squeeze(1) \n    output=torch.max(output, 1).indices\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import confusion_matrix\n    x=confusion_matrix(y_test, output)\n    accuracy = accuracy_score(y_test, output)\n    print(x)\n    print(accuracy)","metadata":{"execution":{"iopub.status.busy":"2022-01-28T07:41:57.490416Z","iopub.execute_input":"2022-01-28T07:41:57.491215Z","iopub.status.idle":"2022-01-28T07:41:57.535619Z","shell.execute_reply.started":"2022-01-28T07:41:57.491173Z","shell.execute_reply":"2022-01-28T07:41:57.534586Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}