{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n#df = pd.read_csv('/kaggle/input/tencentforuse/edit')\ndf=pd.read_csv('/kaggle/input/newclustering/newclasstering.csv')\n#nlp=pd.read_csv('/kaggle/input/nlp-for-ml/NLP.CSV')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-12T03:18:15.619319Z","iopub.execute_input":"2021-12-12T03:18:15.619825Z","iopub.status.idle":"2021-12-12T03:18:15.740172Z","shell.execute_reply.started":"2021-12-12T03:18:15.619738Z","shell.execute_reply":"2021-12-12T03:18:15.739479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nk = np.loadtxt('/kaggle/input/nlpforuse/a.csv',dtype=np.float,delimiter=' ',unpack=False)\nk=k[:2941]\nprint(len(k))","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:18:15.741602Z","iopub.execute_input":"2021-12-12T03:18:15.741826Z","iopub.status.idle":"2021-12-12T03:18:23.815273Z","shell.execute_reply.started":"2021-12-12T03:18:15.741795Z","shell.execute_reply":"2021-12-12T03:18:23.814561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df[['year','topic','status','1','2','play cat']]","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:18:23.817537Z","iopub.execute_input":"2021-12-12T03:18:23.81795Z","iopub.status.idle":"2021-12-12T03:18:23.829901Z","shell.execute_reply.started":"2021-12-12T03:18:23.817909Z","shell.execute_reply":"2021-12-12T03:18:23.829153Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[['status']]=df[['status']].fillna(round(df.mean()))\ndf=df.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:18:23.832393Z","iopub.execute_input":"2021-12-12T03:18:23.832679Z","iopub.status.idle":"2021-12-12T03:18:23.844151Z","shell.execute_reply.started":"2021-12-12T03:18:23.832642Z","shell.execute_reply":"2021-12-12T03:18:23.843468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#from scipy import stats\n#t=stats.zscore(df) \n#t=abs(t) \n#filtered_entries = (t < 3).all(axis=1)\n#new_df = df[filtered_entries]\nnew_df=df","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:18:23.845701Z","iopub.execute_input":"2021-12-12T03:18:23.845958Z","iopub.status.idle":"2021-12-12T03:18:23.852556Z","shell.execute_reply.started":"2021-12-12T03:18:23.845918Z","shell.execute_reply":"2021-12-12T03:18:23.851644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#y_mean_std=(new_df['play number'].mean(),new_df['play number'].std())\n#normalized_df=(new_df-new_df.mean())/new_df.std()\nnew_df['status']=(new_df['status']-new_df['status'].mean())/new_df['status'].std()\nnew_df['1']=(new_df['1']-new_df['1'].mean())/new_df['1'].std()\nnew_df['2']=(new_df['2']-new_df['2'].mean())/new_df['2'].std()\n#new_df['year'].value_counts()\n#from sklearn.preprocessing import LabelBinarizer\n#y = OneHotEncoder().fit_transform(x).toarray()\n#print(y)\n#new_df\n#print(y_mean_std)\nnew_df","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:18:23.854104Z","iopub.execute_input":"2021-12-12T03:18:23.854379Z","iopub.status.idle":"2021-12-12T03:18:23.879755Z","shell.execute_reply.started":"2021-12-12T03:18:23.854342Z","shell.execute_reply":"2021-12-12T03:18:23.878958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"need to deleting outliners","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torchvision import transforms, datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nBATCH_SIZE=8","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:18:23.881125Z","iopub.execute_input":"2021-12-12T03:18:23.88139Z","iopub.status.idle":"2021-12-12T03:18:26.274598Z","shell.execute_reply.started":"2021-12-12T03:18:23.881355Z","shell.execute_reply":"2021-12-12T03:18:26.273875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelBinarizer\nk=torch.tensor(k)\n#print(k.size())\nyear= LabelBinarizer().fit_transform(new_df['year'])\nyear=torch.tensor(year)\ntopic=LabelBinarizer().fit_transform(new_df['topic'])\ntopic=torch.tensor(topic)\n#print(topic[0])\n#print(year.size())\nx= torch.tensor(new_df.iloc[:, 2:-1].values)\n#print(x)\ny=torch.tensor(new_df.iloc[:, -1].values)\ny = y.view(-1, 1)\n#print(y)\ndata = torch.cat((x, year), 1)\ndata = torch.cat((data, topic), 1)\ndata = torch.cat((data, k), 1)\ndata = torch.cat((data, y), 1)\n#print(data.size())\ntrain, test = train_test_split(data, test_size=0.1)\n#print(train.size()[1])\nx_train, y_train=torch.split(train, [train.size()[1]-1,1],dim=1)\nx_test, y_test=torch.split(test, [train.size()[1]-1,1],dim=1)\n#year_train = LabelBinarizer().fit_transform(train['year'])\n#x_train = torch.tensor(train.iloc[:, 0:-2].values)\n#y_train=torch.tensor(train.iloc[:, -1].values)\n#year_train=torch.tensor(year_train)\n#x_train = torch.cat((x_train, year_train), 1)\n\n#year_test = LabelBinarizer().fit_transform(test['year'])\n#x_test = torch.tensor(test.iloc[:, 0:-2].values)\n#y_test=torch.tensor(test.iloc[:, -1].values)\n#year_test=torch.tensor(year_test)\n#x_test = torch.cat((x_test, year_test), 1)\n\nprint(x_train.size())\n#x_test","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:18:26.275886Z","iopub.execute_input":"2021-12-12T03:18:26.276131Z","iopub.status.idle":"2021-12-12T03:18:26.561243Z","shell.execute_reply.started":"2021-12-12T03:18:26.276099Z","shell.execute_reply":"2021-12-12T03:18:26.560494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset1 = TensorDataset(x_train, y_train)\ntrainset = DataLoader(dataset1, batch_size=BATCH_SIZE, shuffle=True)\ndataset2 = TensorDataset(x_test, y_test)\ntestset = DataLoader(dataset2, batch_size=len(y_test), shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:18:26.56237Z","iopub.execute_input":"2021-12-12T03:18:26.563589Z","iopub.status.idle":"2021-12-12T03:18:26.569082Z","shell.execute_reply.started":"2021-12-12T03:18:26.563548Z","shell.execute_reply":"2021-12-12T03:18:26.568352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Net(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc1 = nn.Linear(4483, 1024)\n        self.fc2 = nn.Linear(1024, 256)\n        #self.fc3 = nn.Linear(256, 128)\n        #self.fc4 = nn.Linear(128, 128)\n        #self.fc5 = nn.Linear(128, 128)\n        #self.fc6 = nn.Linear(128, 128)\n        #self.fc7 = nn.Linear(128, 128)\n        self.fc8 = nn.Linear(256, 128)\n        self.fc9 = nn.Linear(128, 3)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        #x = F.relu(self.fc3(x))\n        #x = F.relu(self.fc4(x))\n        #x = F.relu(self.fc5(x))\n        #x = F.relu(self.fc6(x))  \n        #x = F.relu(self.fc7(x))\n        x = F.relu(self.fc8(x))\n        x = self.fc9(x)\n        return x\n\nnet = Net()\nprint(net)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:19:08.438083Z","iopub.execute_input":"2021-12-12T03:19:08.438343Z","iopub.status.idle":"2021-12-12T03:19:08.491091Z","shell.execute_reply.started":"2021-12-12T03:19:08.438315Z","shell.execute_reply":"2021-12-12T03:19:08.490269Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\n#optimizer = optim.Adam(net.parameters(), lr=1e-3)\noptimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.9)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:25:18.011075Z","iopub.execute_input":"2021-12-12T03:25:18.011346Z","iopub.status.idle":"2021-12-12T03:25:18.016747Z","shell.execute_reply.started":"2021-12-12T03:25:18.011317Z","shell.execute_reply":"2021-12-12T03:25:18.015812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(200): # 3 full passes over the data\n    for idx, data in enumerate(trainset):  # `data` is a batch of data\n        #if idx > 0:\n            #continue\n        X, y = data    # X is the batch of features, y is the batch of targets.\n        net.zero_grad()  # sets gradients to 0 before loss calc. You will do this likely every step.\n        #print('x:',X)\n        X=X.clone().detach().float()\n        output = net(X.view(-1,4483))  # pass in the reshaped batch (recall they are 28x28 atm) view(-1,3)\n        #print(y)\n        #y = y.clone().detach().float()\n        #loss = F.nll_loss(output, y.view(-1, 1))\n        y = y.type(torch.LongTensor)\n        y = y.squeeze(1) \n        #print(output,y)\n        loss = criterion(output, y)  # calc and grab the loss value torch.nn.MSELoss\n        #print(loss)\n        loss.backward()  # apply this loss backwards thru the network's parameters\n        optimizer.step()  # attempt to optimize weights to account for loss/gradients\n    #print(output) \n    #print(output,y)\n    if epoch % 5 == 0:\n        #print(y,output)\n        print(epoch,loss)  # print loss. We hope loss (a measure of wrong-ness) declines! \n    if loss<=0.01:\n        break\nprint('end')\n        ","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:25:19.070819Z","iopub.execute_input":"2021-12-12T03:25:19.071765Z","iopub.status.idle":"2021-12-12T03:25:26.324159Z","shell.execute_reply.started":"2021-12-12T03:25:19.071719Z","shell.execute_reply":"2021-12-12T03:25:26.323332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"s=0\nn=0\nfor data in testset:\n    X,y=data\n    X=X.clone().detach().float()\n    output = net(X.view(-1,4483))  # pass in the reshaped batch (recall they are 28x28 atm) view(-1,3)\n        #print(output)\n        #print(y)\n    #y = y.clone().detach().float()\n    y = y.type(torch.LongTensor)\n    y = y.squeeze(1) \n    output=torch.max(output, 1).indices\n    from sklearn.metrics import accuracy_score\n    from sklearn.metrics import confusion_matrix\n    x=confusion_matrix(y_test, output)\n    accuracy = accuracy_score(y_test, output)\n    print(x)\n    print(accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-12-12T03:25:26.32596Z","iopub.execute_input":"2021-12-12T03:25:26.32641Z","iopub.status.idle":"2021-12-12T03:25:26.377103Z","shell.execute_reply.started":"2021-12-12T03:25:26.326366Z","shell.execute_reply":"2021-12-12T03:25:26.376272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}